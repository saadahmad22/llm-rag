# A tutorial on how to use RAG on LLMs 

## Table of contents
1. [Introduction](#introduction)
2. [Setting up an LLM](#llm)
3. [Setting up a Database](#db)
4. [Setting up a RAG app](#rag)

### Introduction <a name="introduction"></a>
First of all, what is RAG? Simply put, RAG is a technique that uses data (in the form of text) to improve results from an LLM model. What this might look like, for example, might be a training chatbot for new hires which uses RAG to sort of "feed" training data (e.g., employee manuals, training video transcripts, etc.) to an LLM. This drastically improves the chatbot's performance, giving it a huge edge over a "base" LLM, which is just using general-purpose information and can take a lot of liberties in interpreting questions and possible answers, since it has not been "trained/taught" on that specific company's training procedure. This is the motivation behind RAG, and below will be a discussion on how to get started. The tutorial will go over doing it locally on a single machine, but bear in mind that scaling it is as simple as deploying the LLM and the RAG app on separate containers.

### Setting up an LLM] <a name="llm"></a>

### Setting up a Database <a name="db"></a>

### Setting up a RAG app <a name="rag"></a>
