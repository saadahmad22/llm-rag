from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages.utils import get_buffer_string
from llama_cpp import Llama

class LlamaModel:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = self.load_model()
        self.chat_histories = {}

    def load_model(self):
        model = Llama(
            model_path=self.model_path,
            n_ctx=2 ** 16,
            n_batch=2 ** 10)
        return model
    
    def get_user_history(self, user_id: str) -> InMemoryChatMessageHistory:
        if user_id not in self.chat_histories:
            self.chat_histories[user_id] = InMemoryChatMessageHistory()
        return self.chat_histories[user_id]

    def generate_response(self, input_data: str, history: InMemoryChatMessageHistory) -> str:
        '''Generates a response to the input data using the Llama model

        Args:
            input_data (str): The input data to generate a response for

        Returns:
            str: The response generated by the model
        '''

        print(input_data)
        input_text = self.preprocess_input(input_data)
        history.add_user_message(input_text)
        response = self.model(prompt=get_buffer_string(history.messages), max_tokens=500)
        history.add_ai_message(response['choices'][0]['text'])
        print(response)
        return self.postprocess_output(response['choices'][0]['text'])

    def preprocess_input(self, input_data: str) -> str:
        # Preprocess the input data before passing it to the model
        return input_data.strip()

    def postprocess_output(self, output_data: str) -> str:
        # Postprocess the output data from the model
        return output_data.strip()